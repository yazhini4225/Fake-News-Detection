!kaggle datasets download -d algord/fake-news
!unzip fake-news.zip

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam

# Load the dataset
df = pd.read_csv("FakeNewsNet.csv")  # Replace with your dataset path

# Extract features and labels
X_data = df["title"].astype(str)  # Use the "title" column
y_data = df["real"].astype(int)   # Use the "real" column as the label

# Tokenize the text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_data)
sequences = tokenizer.texts_to_sequences(X_data)
word_index = tokenizer.word_index
max_sequence_length = 53  # Maximum sequence length based on your dataset

# Pad the sequences
X = pad_sequences(sequences, maxlen=max_sequence_length)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_data, test_size=0.2, random_state=42)

# Load pre-trained GloVe embeddings
embedding_dim = 100  # GloVe dimension (adjust if needed)
embedding_file = "/content/drive/MyDrive/data/glove/glove.6B.100d.txt"  # Replace with the path to GloVe file
embeddings_index = {}

with open(embedding_file, encoding="utf-8") as f:
    for line in f:
        values = line.split()
        word = values[0]
        vector = np.asarray(values[1:], dtype="float32")
        embeddings_index[word] = vector

# Create the embedding matrix
embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

# Build the model
model = Sequential()

# Add the embedding layer with pre-trained GloVe vectors
model.add(Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim,
                    weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))

# Add LSTM layer
model.add(LSTM(100, return_sequences=False))

# Add dropout layer
model.add(Dropout(0.5))

# Add dense layer for binary classification
model.add(Dense(1, activation="sigmoid"))

# Compile the model
model.compile(optimizer=Adam(), loss="binary_crossentropy", metrics=["accuracy"])

# Model summary
model.summary()

# Train the model
history = model.fit(
    X_train,
    y_train,
    epochs=10,  # Number of epochs
    batch_size=32,  # Batch size
    validation_split=0.2,  # Validation split for training data
    verbose=1  # Display training progress
)

# Evaluate the model on test data
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

# Predict on new data 
predictions = model.predict(X_test)
predicted_labels = (predictions > 0.5).astype(int)
